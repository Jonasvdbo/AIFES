{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a21f1771",
   "metadata": {},
   "source": [
    "# Evaluate Performance of model using Backtest Pipeline\n",
    "For the substation Oosterwolde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066796c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "import ipynbname\n",
    "\n",
    "from openstef.pipeline.train_create_forecast_backtest import train_model_and_forecast_back_test\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e658870-3e28-4231-a1d5-e26acbdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "filename = Path(\"../.data/Oosterwolde-10kV.csv\")\n",
    "\n",
    "measurements = pd.read_csv(filename, delimiter=\";\", decimal=\",\")\n",
    "measurements[\"Datetime\"] = pd.to_datetime(measurements[\"Datum\"] + \" \" + measurements[\"Tijd\"])\n",
    "measurements = measurements.set_index('Datetime').tz_localize('CET', ambiguous='NaT', nonexistent='NaT').tz_convert(\"UTC\")\n",
    "# Only keep relevant columns\n",
    "measurements = measurements.iloc[:,2:-1]\n",
    "# Calculate total load\n",
    "measurements['Total'] = measurements.sum(axis=1)\n",
    "# By default, only a backtest will be made for the final column\n",
    "target_column=measurements.columns[-1]\n",
    "\n",
    "measurements.iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad099d2-fa2a-4e6e-bbe6-7b9cb5edb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictors\n",
    "predictors = pd.read_csv('../.data/predictors.csv', index_col=0, parse_dates=True)\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define properties of training/prediction. We call this a 'prediction_job' \n",
    "pj=PredictionJobDataClass(\n",
    "    id=1,\n",
    "    name='TestPrediction',\n",
    "    model='xgb',\n",
    "    quantiles=[0.10,0.30,0.50,0.70,0.90],\n",
    "    horizon_minutes=24*60,\n",
    "    resolution_minutes=15,\n",
    "        \n",
    "    forecast_type=\"demand\", # Note, this should become optional\n",
    "    lat = 1, #should become optional\n",
    "    lon = 1, #should become optional\n",
    "        # train_components=False, #should become optional\n",
    "        #model_type_group=None, # Note, this should become optional\n",
    "        #hyper_params={}, # Note, this should become optional\n",
    "        #feature_names=None, # Note, this should become optional\n",
    "                  )\n",
    "# Define backtest specs\n",
    "backtest_specs = dict(n_folds=3, training_horizons=[0.25, 47.0])\n",
    "\n",
    "\n",
    "modelspecs = ModelSpecificationDataClass(id=pj['id'])\n",
    "\n",
    "# Specify input data, use last column of the load dataframe\n",
    "input_data = pd.DataFrame(dict(load=measurements.loc[:,target_column])).merge(predictors, left_index=True, right_index=True)\n",
    "# Also resample to fix overlappint indices\n",
    "input_data = input_data.resample('15T').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the backtest\n",
    "forecast, model, train_data, validation_data, test_data = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs = modelspecs,\n",
    "    input_data = input_data,\n",
    "    **backtest_specs,\n",
    " )\n",
    "# If n_folds>1, model is a list of models. In that case, only use the first model\n",
    "if backtest_specs['n_folds']>1:\n",
    "    model=model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080d8aa",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs=dict(timeseries=dict())\n",
    "for horizon in set(forecast.horizon):\n",
    "    fig = forecast.loc[forecast.horizon==0.25,['quantile_P10','quantile_P30',\n",
    "                    'quantile_P50','quantile_P70','quantile_P90','realised','forecast']].iplot(asFigure=True,\n",
    "                                                                                   title=f\"Horizon: {horizon}\")\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1), fill='tonexty', fillcolor='rgba(0, 255, 0, 0.1)',\n",
    "         selector=lambda x: 'quantile' in x.name and x.name != 'quantile_P10')\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"green\", width=1),\n",
    "         selector=lambda x: 'quantile_P10' == x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"red\", width=2),\n",
    "         selector=lambda x: 'realised' in x.name)\n",
    "    fig.update_traces(\n",
    "         line=dict(color=\"blue\", width=2),\n",
    "         selector=lambda x: 'forecast' in x.name)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54426833",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['err']=forecast['realised']-forecast['forecast']\n",
    "mae = forecast.pivot_table(index='horizon', values=['err'], aggfunc=lambda x: x.abs().mean())\n",
    "mae.index=mae.index.astype(str)\n",
    "mae_fig = mae.iplot(kind='bar',\n",
    "          layout=dict(title='MAE',\n",
    "                      xaxis=dict(title='horizon'),\n",
    "                      yaxis=dict(title='MAE [MW]')), asFigure=True)\n",
    "mae_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_fig = plot_feature_importance(model.feature_importance_dataframe)\n",
    "feature_importance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb2e34-06d2-4ef0-9633-4c68013dd257",
   "metadata": {},
   "source": [
    "# Store results\n",
    "Store timeseries as csv, metadata as yaml, model as ... and write an overview to pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24417a2f-3cf9-4522-8aa0-bc2fbad4ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f'{datetime.utcnow():%Y%m%d_%H%M%S}_OWD_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adf7d4-529c-4d6f-8134-3042de05fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifacts(run_name, forecast, model, prediction_job, backtest_specs):\n",
    "    \"\"\"Write timeseries to csv and generate PDF of result\"\"\"\n",
    "    \n",
    "    # Create output dir\n",
    "    outdir = Path(f'output/{run_name}')\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "     \n",
    "    # Write forecast_df (includes realised)\n",
    "    forecast.to_csv(outdir / 'forecast.csv', compression='gzip')\n",
    "    \n",
    "    # Write model\n",
    "    model.save_model(outdir / \"model.json\")\n",
    "    \n",
    "    # Write meta data - prediction job and backtest parameters\n",
    "    # relevant prediction_job attributes\n",
    "    rel_attrs = ['id','name','model','quantiles']\n",
    "    rel_pj_dict={key:prediction_job[key] for key in rel_attrs}\n",
    "    with open(outdir / \"configs.yaml\", \"w\") as file:\n",
    "        documents = yaml.dump({**rel_pj_dict, **backtest_specs}, file)\n",
    "\n",
    "write_artifacts(run_name, forecast, model, pj, backtest_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f26d91-27f1-4a67-828d-a2269d7620fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = '01.OWD_Evaluate_performance_using_Backtest_Pipeline.ipynb'\n",
    "command=f\"jupyter nbconvert {nb_fname}--no-input --to html --output results/{run_name}.html\"\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
