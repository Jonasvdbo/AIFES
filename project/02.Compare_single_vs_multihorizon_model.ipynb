{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a21f1771",
   "metadata": {},
   "source": [
    "# Compare training a single model for multiple horizons, versus horizon-dedicated models\n",
    "Normally in OpenSTEF, a single model is trained to forecast load at continuously increasing lead time.\n",
    "\n",
    "However, let's analyse how the accuracy of this setup compares to training models for specific lead times.\n",
    "\n",
    "Conclusion; on the very short term (15 minutes ahead), training a single model for that specific horizon is more accurate. For the longer time horizon, the model trained on both horizons actually outperforms the model only trained on that specific horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066796c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from openstef.pipeline.train_create_forecast_backtest import train_model_and_forecast_back_test\n",
    "from openstef.metrics.figure import plot_feature_importance\n",
    "from openstef.data_classes.model_specifications import ModelSpecificationDataClass\n",
    "from openstef.data_classes.prediction_job import PredictionJobDataClass\n",
    "\n",
    "# Set working dir to location of this file\n",
    "os.chdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e658870-3e28-4231-a1d5-e26acbdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name\n",
    "nb_title = 'Compare_single_vs_multihorizon_model'\n",
    "\n",
    "# Load inputs\n",
    "filename = Path(\"../.data/Middenmeer-150kV.csv\")\n",
    "\n",
    "measurements = pd.read_csv(filename, delimiter=\";\", decimal=\",\")\n",
    "measurements[\"Datetime\"] = pd.to_datetime(measurements[\"Datum\"] + \" \" + measurements[\"Tijd\"])\n",
    "measurements = measurements.set_index('Datetime').tz_localize('CET', ambiguous='NaT', nonexistent='NaT').tz_convert(\"UTC\")\n",
    "# Only keep relevant columns\n",
    "measurements = measurements.iloc[:,2:-1]\n",
    "# Sum the load\n",
    "measurements['Total'] = measurements.sum(axis=1)\n",
    "# By default, only a backtest is made for the total\n",
    "target_column = 'Total'\n",
    "\n",
    "measurements.iplot(layout=dict(template='plotly_white'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad099d2-fa2a-4e6e-bbe6-7b9cb5edb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictors\n",
    "predictors = pd.read_csv('../.data/predictors.csv', index_col=0, parse_dates=True)\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties of training/prediction. We call this a 'prediction_job' \n",
    "pj=PredictionJobDataClass(\n",
    "    id=1,\n",
    "    name='TestPrediction',\n",
    "    model='xgb',\n",
    "    quantiles=[0.10,0.30,0.50,0.70,0.90],\n",
    "    horizon_minutes=24*60,\n",
    "    resolution_minutes=15,\n",
    "        \n",
    "    forecast_type=\"demand\", # Note, this should become optional\n",
    "    lat = 1, #should become optional\n",
    "    lon = 1, #should become optional\n",
    "                  )\n",
    "\n",
    "training_horizons=[0.25, 47.0]\n",
    "\n",
    "# Make backtest using a single model for all lead times\n",
    "# Define backtest specs\n",
    "backtest_specs = dict(n_folds=3, training_horizons=training_horizons)\n",
    "modelspecs = ModelSpecificationDataClass(id=pj['id'])\n",
    "\n",
    "# Specify input data, use last column of the load dataframe\n",
    "input_data = pd.DataFrame(dict(load=measurements.loc[:,target_column])).merge(predictors, left_index=True, right_index=True)\n",
    "# Also resample to fix overlapping indices\n",
    "input_data = input_data.resample('15T').mean()\n",
    "\n",
    "\n",
    "# Perform the backtest\n",
    "forecast_single_model, model_single_model, train_data, validation_data, test_data = train_model_and_forecast_back_test(\n",
    "    pj,\n",
    "    modelspecs = modelspecs,\n",
    "    input_data = input_data,\n",
    "    **backtest_specs,\n",
    " )\n",
    "\n",
    "# Store the model, so it can be compared to the other models\n",
    "models=dict(multihorizonmodel=model_single_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat backtest, but now with seperate models for each horizon\n",
    "forecast_dedicated_model = pd.DataFrame()\n",
    "models_dedicated_model = dict()\n",
    "for horizon in training_horizons:\n",
    "    forecast, model, train_data, validation_data, test_data = train_model_and_forecast_back_test(\n",
    "        pj,\n",
    "        modelspecs = modelspecs,\n",
    "        input_data = input_data,\n",
    "        **dict(n_folds=backtest_specs['n_folds'], training_horizons=[horizon]),\n",
    "    )\n",
    "    forecast_dedicated_model = forecast_dedicated_model.append(forecast)\n",
    "    models.update({f'dedicated_model_{horizon}': model})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7080d8aa",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine. df should have the P50 forecast for single/multimodel and for short/long horizon\n",
    "df = pd.DataFrame(dict(forecast_multihorizonmodel_short=forecast_single_model[forecast_single_model.horizon==0.25]['forecast'].values,\n",
    "                       forecast_multihorizonmodel_long =forecast_single_model[forecast_single_model.horizon==47.0]['forecast'].values,\n",
    "                       forecast_dedicatedmodels_short = forecast_dedicated_model[forecast_dedicated_model.horizon==0.25]['forecast'].values,\n",
    "                       forecast_dedicatedmodels_long = forecast_dedicated_model[forecast_dedicated_model.horizon==47.0]['forecast'].values,\n",
    "                       realised = forecast_dedicated_model[forecast_dedicated_model.horizon==47.0]['realised'].values,\n",
    "                       ),\n",
    "                  index = forecast_dedicated_model[forecast_dedicated_model.horizon==47.0].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df = df.apply(lambda x: x-x.realised, axis=1)\n",
    "err_df.iloc[:,:-1].abs().mean()[[0,2,1,3]].iplot(kind='bar', yTitle='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot feature importances of models - size = gain, color = weight.\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Name: {name}')\n",
    "    feature_importance_fig = plot_feature_importance(model[0].feature_importance_dataframe)\n",
    "    feature_importance_fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c2579be",
   "metadata": {},
   "source": [
    "More information on feature importance; the size indicates gain, the color indicates weight. References: [ref1](https://datascience.stackexchange.com/questions/12318/how-to-interpret-the-output-of-xgboost-importance) [ref2](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7)\n",
    "\n",
    "The Gain is the most relevant attribute to interpret the relative importance of each feature.\n",
    "\n",
    "‘Gain’ is the improvement in accuracy brought by a feature to the branches it is on. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branch is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite).\n",
    "\n",
    "‘weight’: the number of times a feature is used to split the data across all trees.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65bb2e34-06d2-4ef0-9633-4c68013dd257",
   "metadata": {},
   "source": [
    "# Store results\n",
    "Store timeseries as csv, metadata as yaml, model as ... and write an overview to pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24417a2f-3cf9-4522-8aa0-bc2fbad4ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f'{datetime.utcnow():%Y%m%d_%H%M%S}_{nb_title}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adf7d4-529c-4d6f-8134-3042de05fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifacts(run_name, forecast, model, prediction_job, backtest_specs):\n",
    "    \"\"\"Write timeseries to csv and generate PDF of result\"\"\"\n",
    "    \n",
    "    # Create output dir\n",
    "    outdir = Path(f'output/{run_name}')\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "     \n",
    "    # Write forecast_df (includes realised)\n",
    "    forecast.to_csv(outdir / 'forecast.csv', compression='gzip')\n",
    "    \n",
    "    # Write model\n",
    "    model.save_model(outdir / \"model.json\")\n",
    "    \n",
    "    # Write meta data - prediction job and backtest parameters\n",
    "    # relevant prediction_job attributes\n",
    "    rel_attrs = ['id','name','model','quantiles']\n",
    "    rel_pj_dict={key:prediction_job[key] for key in rel_attrs}\n",
    "    with open(outdir / \"configs.yaml\", \"w\") as file:\n",
    "        documents = yaml.dump({**rel_pj_dict, **backtest_specs}, file)\n",
    "\n",
    "write_artifacts(run_name, forecast, model[0], pj, backtest_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d6a30-b18d-4c7c-8fa2-2f3e67da3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = '02.Compare_single_vs_multihorizon_model'\n",
    "command=f\"jupyter nbconvert {nb_fname}.ipynb --to html --output results/{nb_title}.html\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f26d91-27f1-4a67-828d-a2269d7620fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fadd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
